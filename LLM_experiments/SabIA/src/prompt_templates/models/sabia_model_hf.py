from transformers import AutoTokenizer, LlamaForCausalLM
from framework_classes.completion_model import CompletionModel

class SabIAModelHf(CompletionModel):
    """
    Specialization of a `CompletionModel` using the LLaMA model hosted on Hugging Face.
    Functions:
        - predict(): receives an input from the user and the model returns the response.
    """

    def __init__(self, model_name: str = "meta-llama/Llama-2-7b-hf") -> None:
        """
        Initializes the model with a specific LLaMA model from Hugging Face.
        Parameters:
            - model_name (str): The model identifier on the Hugging Face Model Hub.
        """
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = LlamaForCausalLM.from_pretrained(model_name)

    def predict(self, message: str, max_length: int = 30) -> str:
        """
        Receives an input from the user and the model returns the response.
        Parameters:
            - message (str): User input (question)
            - max_length (int): Maximum length of the generated response
        Return:
            - Response generated by the LLM
        """
        # Prepare the input text
        inputs = self.tokenizer(message, return_tensors="pt", max_length=max_length, truncation=True)

        # Generate response
        output_sequences = self.model.generate(
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            max_length=max_length,
        )

        # Decode and return the response
        response = self.tokenizer.decode(output_sequences[0], skip_special_tokens=True)
        return response
