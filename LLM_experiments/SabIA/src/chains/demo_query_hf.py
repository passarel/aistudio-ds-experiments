from transformers import AutoTokenizer, LlamaForCausalLM
from framework_classes.completion_model import CompletionModel

class DemoModelHf(CompletionModel):
    def __init__(self, model_name: str = "meta-llama/Llama-2-7b-hf") -> None:
        """
        Initializes the model with a specific LLaMA model from Hugging Face and optional parameters for generation.
        Parameters:
            - model_name (str): The model identifier on the Hugging Face Model Hub.
            - **kwargs: Additional parameters for text generation, such as temperature.
        """
        self.model_name = model_name
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = LlamaForCausalLM.from_pretrained(model_name)
        self.generation_kwargs = kwargs

    def predict(self, message: str) -> str:
        """
        Receives an input from the user and the model returns the response.
        Parameters:
            - message (str): User input (question)
        Return:
            - Response generated by the LLM
        """
        # Prepare the input text
        inputs = self.tokenizer(message, return_tensors="pt")

        # Generate response
        output_sequences = self.model.generate(
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            **self.generation_kwargs
        )

        # Decode and return the response
        response = self.tokenizer.decode(output_sequences[0], skip_special_tokens=True)
        return response
