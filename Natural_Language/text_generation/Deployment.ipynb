{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b3f7a87-1faf-4f44-8b89-b7be46ffcc2b",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c12dca25-6a47-4674-aa23-8dd9e7afbdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow import MlflowClient\n",
    "from mlflow.types.schema import Schema, ColSpec\n",
    "from mlflow.types import ParamSchema, ParamSpec\n",
    "from mlflow.models import ModelSignature\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89849458-6614-41f7-916c-ca38aec363eb",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4e89ba8-1e88-450e-bea4-4a0a99793bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/shakespeare.txt','r',encoding='utf8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "all_characters = set(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5e8c88-c69f-416a-9ac0-04f049988fc2",
   "metadata": {},
   "source": [
    "### Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "146ac375-2b3d-431c-a186-d1372e106cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, decoder, encoder, all_chars, num_hidden=256, num_layers=4,drop_prob=0.5, use_gpu=False):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.num_layers = num_layers\n",
    "        self.num_hidden = num_hidden\n",
    "        self.use_gpu = use_gpu\n",
    "        \n",
    "        self.all_chars = all_chars\n",
    "        self.decoder = torch.load(decoder)\n",
    "        self.encoder = torch.load(encoder)\n",
    "        \n",
    "        self.lstm = nn.LSTM(len(self.all_chars), num_hidden, num_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc_linear = nn.Linear(num_hidden, len(self.all_chars))\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        lstm_output, hidden = self.lstm(x, hidden)       \n",
    "        drop_output = self.dropout(lstm_output)\n",
    "        drop_output = drop_output.contiguous().view(-1, self.num_hidden)\n",
    "        final_out = self.fc_linear(drop_output)\n",
    "        \n",
    "        return final_out, hidden\n",
    "    \n",
    "    \n",
    "    def hidden_state(self, batch_size):\n",
    "        if self.use_gpu:\n",
    "            hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden).cuda(),\n",
    "                     torch.zeros(self.num_layers,batch_size,self.num_hidden).cuda())\n",
    "        else:\n",
    "            hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden),\n",
    "                     torch.zeros(self.num_layers,batch_size,self.num_hidden))\n",
    "        \n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df33904-7221-4be1-846d-6f7a24c58cb9",
   "metadata": {},
   "source": [
    "# MLFlow - Register Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3c2d2faa-e9d5-45b3-a862-8c6136bbe625",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(mlflow.pyfunc.PythonModel):\n",
    "    \n",
    "    def load_context(self, context):\n",
    "        self.model = CharModel(\n",
    "                        all_chars=all_characters,\n",
    "                        num_hidden=512,\n",
    "                        num_layers=3,\n",
    "                        drop_prob=0.5,\n",
    "                        use_gpu=False,\n",
    "                        decoder=context.artifacts['decoder'],\n",
    "                        encoder=context.artifacts['encoder']\n",
    "                                           \n",
    "                    )\n",
    "\n",
    "\n",
    "        self.model.load_state_dict(torch.load(context.artifacts['model_state_dict']))\n",
    "        self.model.eval()\n",
    "\n",
    "    def one_hot_encoder(self, encoded_text, num_uni_chars):\n",
    "        one_hot = np.zeros((encoded_text.size, num_uni_chars))\n",
    "        one_hot = one_hot.astype(np.float32)\n",
    "        one_hot[np.arange(one_hot.shape[0]), encoded_text.flatten()] = 1.0\n",
    "        one_hot = one_hot.reshape((*encoded_text.shape, num_uni_chars))\n",
    "        \n",
    "        return one_hot\n",
    "\n",
    "    def predict_next_char(self, char, hidden=None, k=3):\n",
    "        encoded_text = self.model.encoder[char]\n",
    "        encoded_text = np.array([[encoded_text]])\n",
    "        encoded_text = self.one_hot_encoder(encoded_text, len(self.model.all_chars))\n",
    "        inputs = torch.from_numpy(encoded_text)\n",
    "        inputs = inputs.cpu()\n",
    "            \n",
    "        hidden = tuple([state.data for state in hidden])\n",
    "        lstm_out, hidden = self.model(inputs, hidden)    \n",
    "        probs = F.softmax(lstm_out, dim=1).data\n",
    "        probs = probs.cpu()\n",
    "\n",
    "        \n",
    "        probs, index_positions = probs.topk(k)        \n",
    "        index_positions = index_positions.numpy().squeeze()\n",
    "        probs = probs.numpy().flatten()\n",
    "        probs = probs/probs.sum()\n",
    "        char = np.random.choice(index_positions, p=probs)\n",
    "    \n",
    "        return self.model.decoder[char], hidden\n",
    "\n",
    "    def generate_text(self, seed, size, k=3):\n",
    "\n",
    "        self.model.cpu()\n",
    "            \n",
    "        self.model.eval()\n",
    "        output_chars = [c for c in seed]\n",
    "        hidden = self.model.hidden_state(1)\n",
    "        \n",
    "        for char in seed:\n",
    "            char, hidden = self.predict_next_char(char, hidden, k=k)\n",
    "    \n",
    "        output_chars.append(char)\n",
    "        for i in range(size):\n",
    "            char, hidden = self.predict_next_char(output_chars[-1], hidden, k=k)\n",
    "            output_chars.append(char)\n",
    "            \n",
    "        return ''.join(output_chars)\n",
    "            \n",
    "        \n",
    "    def predict(self, context, model_input):\n",
    "        initial_word = model_input['initial_word'][0]\n",
    "        size = model_input['size'][0]\n",
    "        output = self.generate_text(seed=initial_word, size=size)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    @classmethod\n",
    "    def log_model(cls, model_state_dict, decoder, encoder): \n",
    "        input_schema = Schema(\n",
    "            [\n",
    "                ColSpec(\"string\", \"initial_word\"),\n",
    "                ColSpec(\"long\", \"size\")\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        output_schema = Schema(\n",
    "            [\n",
    "                ColSpec(\"string\", \"generated_text\")\n",
    "            ]\n",
    "        )\n",
    "      \n",
    "        signature = ModelSignature(inputs=input_schema, outputs=output_schema)\n",
    "             \n",
    "        requirements = [\n",
    "            \"mlflow==2.6.0\",\n",
    "            \"torch==2.0.0\",\n",
    "            \"numpy==1.24.3\"\n",
    "        ]\n",
    "        mlflow.pyfunc.log_model(\n",
    "            model_state_dict,\n",
    "            python_model=cls(),\n",
    "            artifacts={\"model_state_dict\": model_state_dict, 'decoder': decoder, 'encoder': encoder},\n",
    "            signature=signature,\n",
    "            pip_requirements=requirements\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "88808f17-797b-4273-86a8-ec4be7b67bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='/phoenix/mlflow/886524021752710328', creation_time=1712950963927, experiment_id='886524021752710328', last_update_time=1712950963927, lifecycle_stage='active', name='Text Generation with RNN', tags={}>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment(experiment_name='Text Generation with RNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d0e28b95-5dd5-4f36-adb3-258a85edc5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state_dict = 'models/dict_torch_rnn_model.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dbf659ed-12e3-4941-b46f-d8cacfdfa5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "register_name = 'rnn_model_deployment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "344da6ed-dd4a-4063-86ac-f6f0168ffb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run's Artifact URI: /phoenix/mlflow/886524021752710328/e130d630eddc48f99c59eb8aacae214d/artifacts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd37a91f1df34411b54845f321e20b8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a994c7f7f89a451ba0998a5019808c9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34e39b39235240bd99c5ec45c3ba8fa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/_distutils_hack/__init__.py:11: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/_distutils_hack/__init__.py:26: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "Registered model 'rnn_model_deployment' already exists. Creating a new version of this model...\n",
      "2024/04/23 14:33:54 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: rnn_model_deployment, version 37\n",
      "Created version '37' of model 'rnn_model_deployment'.\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name='RNN with Torch') as run:\n",
    "    print(f\"Run's Artifact URI: {run.info.artifact_uri}\")\n",
    "    RNNModel.log_model(model_state_dict, 'models/decoder.pt', 'models/encoder.pt')\n",
    "    mlflow.register_model(model_uri = f\"runs:/{run.info.run_id}/{model_state_dict}\", name=register_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3bc4e7fa-fda9-422d-bff6-b077b6c8d3c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = mlflow.MlflowClient()\n",
    "model_metadata = client.get_latest_versions(register_name, stages=[\"None\"])\n",
    "latest_model_version = model_metadata[0].version\n",
    "latest_model_version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923cf27e-2a84-469c-8fe7-ab2508a4a4d4",
   "metadata": {},
   "source": [
    "#### Testing registered model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2caeb0bd-e530-461f-a30a-1a918f85a63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.state_dict of CharModel(\n",
      "  (lstm): LSTM(84, 512, num_layers=3, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc_linear): Linear(in_features=512, out_features=84, bias=True)\n",
      ")>\n",
      "Love to him; and\n",
      "    I have seen him this, though I have sent my heart\n",
      "    And we that bring the willing f\n"
     ]
    }
   ],
   "source": [
    "model = mlflow.pyfunc.load_model(model_uri=f\"models:/{register_name}/{latest_model_version}\")\n",
    "print(model.predict({\"initial_word\": 'Love ', \"size\": 100}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
