{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RYGnI-EZp_nK"
   },
   "source": [
    "# Getting Started: Sample Conversational AI application\n",
    "This notebook shows how to use NVIDIA NeMo (https://github.com/NVIDIA/NeMo) to construct a toy demo which translate Mandarin audio file into English one.\n",
    "\n",
    "The demo demonstrates how to: \n",
    "\n",
    "* Instantiate pre-trained NeMo models from NVIDIA NGC.\n",
    "* Transcribe audio with English speech recognition model.\n",
    "* Translate text to Spanish with machine translation model.\n",
    "* Generate audio with text-to-speech models fine-tuned to Spanish speach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EyJ5HiiPrPKA"
   },
   "source": [
    "## Import all necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "tdUqxeUEA8nw"
   },
   "outputs": [],
   "source": [
    "# Import NeMo and it's ASR, NLP and TTS collections\n",
    "import nemo\n",
    "# Import Speech Recognition collection\n",
    "import nemo.collections.asr as nemo_asr\n",
    "# Import Natural Language Processing collection\n",
    "import nemo.collections.nlp as nemo_nlp\n",
    "# Import Speech Synthesis collection\n",
    "import nemo.collections.tts as nemo_tts\n",
    "# We'll use this to listen to audio\n",
    "import IPython\n",
    "\n",
    "import soundfile\n",
    "import uuid\n",
    "import io\n",
    "import base64\n",
    "import json\n",
    "import logging\n",
    "\n",
    "import mlflow\n",
    "import os\n",
    "from mlflow.types.schema import Schema, ColSpec\n",
    "from mlflow.types import ParamSchema, ParamSpec\n",
    "from mlflow.models import ModelSignature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bt2EZyU3A1aq"
   },
   "source": [
    "## Test whether NeMo is properly imported\n",
    "\n",
    "In this cell, we show a list of available NeMo models for Automatic Speech Recognition on NGC, to show our Workspace is capable to load NeMo and connect to NGC\n",
    "\n",
    "* ``list_available_models()`` - it will list all models currently available on NGC and their names.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "YNNHs5Xjr8ox",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PretrainedModelInfo(\n",
       " \tpretrained_model_name=QuartzNet15x5Base-En,\n",
       " \tdescription=QuartzNet15x5 model trained on six datasets: LibriSpeech, Mozilla Common Voice (validated clips from en_1488h_2019-12-10), WSJ, Fisher, Switchboard, and NSC Singapore English. It was trained with Apex/Amp optimization level O1 for 600 epochs. The model achieves a WER of 3.79% on LibriSpeech dev-clean, and a WER of 10.05% on dev-other. Please visit https://ngc.nvidia.com/catalog/models/nvidia:nemospeechmodels for further details.,\n",
       " \tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemospeechmodels/versions/1.0.0a5/files/QuartzNet15x5Base-En.nemo\n",
       " ),\n",
       " PretrainedModelInfo(\n",
       " \tpretrained_model_name=stt_en_quartznet15x5,\n",
       " \tdescription=For details about this model, please visit https://ngc.nvidia.com/catalog/models/nvidia:nemo:stt_en_quartznet15x5,\n",
       " \tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_en_quartznet15x5/versions/1.0.0rc1/files/stt_en_quartznet15x5.nemo\n",
       " ),\n",
       " PretrainedModelInfo(\n",
       " \tpretrained_model_name=stt_en_jasper10x5dr,\n",
       " \tdescription=For details about this model, please visit https://ngc.nvidia.com/catalog/models/nvidia:nemo:stt_en_jasper10x5dr,\n",
       " \tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_en_jasper10x5dr/versions/1.0.0rc1/files/stt_en_jasper10x5dr.nemo\n",
       " ),\n",
       " PretrainedModelInfo(\n",
       " \tpretrained_model_name=stt_ca_quartznet15x5,\n",
       " \tdescription=For details about this model, please visit https://ngc.nvidia.com/catalog/models/nvidia:nemo:stt_ca_quartznet15x5,\n",
       " \tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_ca_quartznet15x5/versions/1.0.0rc1/files/stt_ca_quartznet15x5.nemo\n",
       " ),\n",
       " PretrainedModelInfo(\n",
       " \tpretrained_model_name=stt_it_quartznet15x5,\n",
       " \tdescription=For details about this model, please visit https://ngc.nvidia.com/catalog/models/nvidia:nemo:stt_it_quartznet15x5,\n",
       " \tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_it_quartznet15x5/versions/1.0.0rc1/files/stt_it_quartznet15x5.nemo\n",
       " ),\n",
       " PretrainedModelInfo(\n",
       " \tpretrained_model_name=stt_fr_quartznet15x5,\n",
       " \tdescription=For details about this model, please visit https://ngc.nvidia.com/catalog/models/nvidia:nemo:stt_fr_quartznet15x5,\n",
       " \tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_fr_quartznet15x5/versions/1.0.0rc1/files/stt_fr_quartznet15x5.nemo\n",
       " ),\n",
       " PretrainedModelInfo(\n",
       " \tpretrained_model_name=stt_es_quartznet15x5,\n",
       " \tdescription=For details about this model, please visit https://ngc.nvidia.com/catalog/models/nvidia:nemo:stt_es_quartznet15x5,\n",
       " \tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_es_quartznet15x5/versions/1.0.0rc1/files/stt_es_quartznet15x5.nemo\n",
       " ),\n",
       " PretrainedModelInfo(\n",
       " \tpretrained_model_name=stt_de_quartznet15x5,\n",
       " \tdescription=For details about this model, please visit https://ngc.nvidia.com/catalog/models/nvidia:nemo:stt_de_quartznet15x5,\n",
       " \tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_de_quartznet15x5/versions/1.0.0rc1/files/stt_de_quartznet15x5.nemo\n",
       " ),\n",
       " PretrainedModelInfo(\n",
       " \tpretrained_model_name=stt_pl_quartznet15x5,\n",
       " \tdescription=For details about this model, please visit https://ngc.nvidia.com/catalog/models/nvidia:nemo:stt_pl_quartznet15x5,\n",
       " \tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_pl_quartznet15x5/versions/1.0.0rc1/files/stt_pl_quartznet15x5.nemo\n",
       " ),\n",
       " PretrainedModelInfo(\n",
       " \tpretrained_model_name=stt_ru_quartznet15x5,\n",
       " \tdescription=For details about this model, please visit https://ngc.nvidia.com/catalog/models/nvidia:nemo:stt_ru_quartznet15x5,\n",
       " \tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_ru_quartznet15x5/versions/1.0.0rc1/files/stt_ru_quartznet15x5.nemo\n",
       " ),\n",
       " PretrainedModelInfo(\n",
       " \tpretrained_model_name=stt_zh_citrinet_512,\n",
       " \tdescription=For details about this model, please visit https://ngc.nvidia.com/catalog/models/nvidia:nemo:stt_zh_citrinet_512,\n",
       " \tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_zh_citrinet_512/versions/1.0.0rc1/files/stt_zh_citrinet_512.nemo\n",
       " ),\n",
       " PretrainedModelInfo(\n",
       " \tpretrained_model_name=stt_zh_citrinet_1024_gamma_0_25,\n",
       " \tdescription=For details about this model, please visit https://ngc.nvidia.com/catalog/models/nvidia:nemo:stt_zh_citrinet_1024_gamma_0_25,\n",
       " \tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_zh_citrinet_1024_gamma_0_25/versions/1.0.0/files/stt_zh_citrinet_1024_gamma_0_25.nemo\n",
       " ),\n",
       " PretrainedModelInfo(\n",
       " \tpretrained_model_name=stt_zh_citrinet_1024_gamma_0_25,\n",
       " \tdescription=For details about this model, please visit https://ngc.nvidia.com/catalog/models/nvidia:nemo:stt_zh_citrinet_1024_gamma_0_25,\n",
       " \tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_zh_citrinet_1024_gamma_0_25/versions/1.0.0/files/stt_zh_citrinet_1024_gamma_0_25.nemo\n",
       " ),\n",
       " PretrainedModelInfo(\n",
       " \tpretrained_model_name=asr_talknet_aligner,\n",
       " \tdescription=For details about this model, please visit https://ngc.nvidia.com/catalog/models/nvidia:nemo:asr_talknet_aligner,\n",
       " \tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/asr_talknet_aligner/versions/1.0.0rc1/files/qn5x5_libri_tts_phonemes.nemo\n",
       " )]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here is an example of all CTC-based models:\n",
    "nemo_asr.models.EncDecCTCModel.list_available_models()\n",
    "# More ASR Models are available - see: nemo_asr.models.ASRModel.list_available_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading from local saved models\n",
    "\n",
    "Here, instead of downloading the models directly from NGC via code, we are showing that we can access the models that were downloaded previously, using Ai Studio assets manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-02-29 20:08:11 deprecated:63] Function ``g2p_backward_compatible_support`` is deprecated. But it will not be removed until a further notice. G2P object root directory `nemo_text_processing.g2p` has been replaced with `nemo.collections.tts.g2p`. Please use the latter instead as of NeMo 1.18.0.\n",
      "[NeMo W 2024-02-29 20:08:11 experimental:26] `<class 'nemo.collections.tts.g2p.models.i18n_ipa.IpaG2p'>` is experimental and not ready for production yet. Use at your own risk.\n",
      "[NeMo W 2024-02-29 20:08:12 i18n_ipa:124] apply_to_oov_word=None, This means that some of words will remain unchanged if they are not handled by any of the rules in self.parse_one_word(). This may be intended if phonemes and chars are both valid inputs, otherwise, you may see unexpected deletions in your input.\n",
      "[NeMo W 2024-02-29 20:08:12 experimental:26] `<class 'nemo.collections.common.tokenizers.text_to_speech.tts_tokenizers.IPATokenizer'>` is experimental and not ready for production yet. Use at your own risk.\n",
      "[NeMo W 2024-02-29 20:08:12 modelPT:161] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    dataset:\n",
      "      _target_: nemo.collections.tts.torch.data.TTSDataset\n",
      "      manifest_filepath: /home/rlangman/Data/openslr/spanish/ipa/train_fastpitch_manifest.json\n",
      "      sample_rate: 44100\n",
      "      sup_data_path: /home/rlangman/Data/openslr/spanish/ipa/sup_data\n",
      "      sup_data_types:\n",
      "      - align_prior_matrix\n",
      "      - pitch\n",
      "      - speaker_id\n",
      "      n_fft: 2048\n",
      "      win_length: 2048\n",
      "      hop_length: 512\n",
      "      window: hann\n",
      "      n_mels: 80\n",
      "      lowfreq: 0\n",
      "      highfreq: null\n",
      "      max_duration: 15\n",
      "      min_duration: 0.1\n",
      "      ignore_file: null\n",
      "      trim: false\n",
      "      pitch_fmin: 65.40639132514966\n",
      "      pitch_fmax: 2093.004522404789\n",
      "      pitch_norm: true\n",
      "      pitch_stats_path: /home/rlangman/Code/NeMo/scripts/tts_dataset_files/openslr_es/pitch_stats.json\n",
      "    dataloader_params:\n",
      "      drop_last: false\n",
      "      shuffle: true\n",
      "      batch_size: 32\n",
      "      num_workers: 12\n",
      "      pin_memory: true\n",
      "    \n",
      "[NeMo W 2024-02-29 20:08:12 modelPT:168] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    dataset:\n",
      "      _target_: nemo.collections.tts.torch.data.TTSDataset\n",
      "      manifest_filepath: /home/rlangman/Data/openslr/spanish/ipa/val_fastpitch_manifest.json\n",
      "      sample_rate: 44100\n",
      "      sup_data_path: /home/rlangman/Data/openslr/spanish/ipa/sup_data\n",
      "      sup_data_types:\n",
      "      - align_prior_matrix\n",
      "      - pitch\n",
      "      - speaker_id\n",
      "      n_fft: 2048\n",
      "      win_length: 2048\n",
      "      hop_length: 512\n",
      "      window: hann\n",
      "      n_mels: 80\n",
      "      lowfreq: 0\n",
      "      highfreq: null\n",
      "      max_duration: 15\n",
      "      min_duration: 0.1\n",
      "      ignore_file: null\n",
      "      trim: false\n",
      "      pitch_fmin: 65.40639132514966\n",
      "      pitch_fmax: 2093.004522404789\n",
      "      pitch_norm: true\n",
      "      pitch_stats_path: /home/rlangman/Code/NeMo/scripts/tts_dataset_files/openslr_es/pitch_stats.json\n",
      "    dataloader_params:\n",
      "      drop_last: false\n",
      "      shuffle: false\n",
      "      batch_size: 32\n",
      "      num_workers: 8\n",
      "      pin_memory: true\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-02-29 20:08:12 features:289] PADDING: 1\n",
      "[NeMo I 2024-02-29 20:08:13 save_restore_connector:249] Model FastPitchModel was successfully restored from /home/jovyan/datafabric/FastPitch-HiFiGAN/tts_es_fastpitch_multispeaker.nemo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-02-29 20:08:27 modelPT:161] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    dataset:\n",
      "      _target_: nemo.collections.tts.torch.data.VocoderDataset\n",
      "      manifest_filepath: /home/rlangman/Data/openslr/spanish/ipa/train_hifi_gta_manifest.json\n",
      "      sample_rate: 44100\n",
      "      n_segments: 16384\n",
      "      max_duration: null\n",
      "      min_duration: 0.75\n",
      "      load_precomputed_mel: true\n",
      "      hop_length: 512\n",
      "    dataloader_params:\n",
      "      drop_last: false\n",
      "      shuffle: true\n",
      "      batch_size: 16\n",
      "      num_workers: 4\n",
      "      pin_memory: true\n",
      "    \n",
      "[NeMo W 2024-02-29 20:08:27 modelPT:168] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    dataset:\n",
      "      _target_: nemo.collections.tts.torch.data.VocoderDataset\n",
      "      manifest_filepath: /home/rlangman/Data/openslr/spanish/ipa/val_hifi_gta_manifest.json\n",
      "      sample_rate: 44100\n",
      "      n_segments: 131072\n",
      "      max_duration: null\n",
      "      min_duration: 3\n",
      "      load_precomputed_mel: true\n",
      "      hop_length: 512\n",
      "    dataloader_params:\n",
      "      drop_last: false\n",
      "      shuffle: false\n",
      "      batch_size: 16\n",
      "      num_workers: 4\n",
      "      pin_memory: true\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-02-29 20:08:27 features:289] PADDING: 0\n",
      "[NeMo I 2024-02-29 20:08:27 features:297] STFT using exact pad\n",
      "[NeMo I 2024-02-29 20:08:27 features:289] PADDING: 0\n",
      "[NeMo I 2024-02-29 20:08:27 features:297] STFT using exact pad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-02-29 20:08:27 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "      warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-02-29 20:08:29 save_restore_connector:249] Model HifiGanModel was successfully restored from /home/jovyan/datafabric/FastPitch-HiFiGAN/tts_es_hifigan_ft_fastpitch_multispeaker.nemo.\n"
     ]
    }
   ],
   "source": [
    "# Speech Recognition model - Citrinet initially trained on Multilingual LibriSpeech English corpus, and fine-tuned on the open source Aishell-2\n",
    "asr_model = nemo_asr.models.EncDecCTCModel.restore_from(\"/home/jovyan/datafabric/STT-Citrinet/stt_en_citrinet_1024_gamma_0_25.nemo\")\n",
    "\n",
    "# Neural Machine Translation model\n",
    "nmt_model = nemo_nlp.models.MTEncDecModel.restore_from(\"/home/jovyan//datafabric/en-es-transformer/nmt_en_es_transformer12x2.nemo\")\n",
    "\n",
    "# Spectrogram generator which takes text as an input and produces spectrogram\n",
    "spectrogram_generator = nemo_tts.models.FastPitchModel.restore_from(\"/home/jovyan/datafabric/FastPitch-HiFiGAN/tts_es_fastpitch_multispeaker.nemo\")\n",
    "\n",
    "# Vocoder model which takes spectrogram and produces actual audio\n",
    "vocoder = nemo_tts.models.HifiGanModel.restore_from(\"/home/jovyan/datafabric/FastPitch-HiFiGAN/tts_es_hifigan_ft_fastpitch_multispeaker.nemo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_sample = 'ForrestGump.mp3'\n",
    "# To listen it, click on the play button below\n",
    "IPython.display.Audio(audio_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcribed_text = asr_model.transcribe([audio_sample])\n",
    "print(transcribed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_nmt_model = nmt_model.cuda()\n",
    "translated_text = cuda_nmt_model.translate(transcribed_text)\n",
    "\n",
    "print(translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_spectrogram_generator = spectrogram_generator.cuda()\n",
    "cuda_vocoder = vocoder.cuda()\n",
    "\n",
    "\n",
    "parsed = cuda_spectrogram_generator.parse(translated_text[0])\n",
    "spectrogram = cuda_spectrogram_generator.generate_spectrogram(tokens=parsed, speaker=2)\n",
    "audio = cuda_vocoder.convert_spectrogram_to_audio(spec=spectrogram)\n",
    "IPython.display.Audio(audio.to('cpu').detach().numpy(), rate=44100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NemoTranslationModel(mlflow.pyfunc.PythonModel):\n",
    "    def transcribe_audio(self, inputs):\n",
    "        \"\"\"\n",
    "        Deserializes base64-serialized audio to a NumPy array.\n",
    "        Assume the audio is in WAV format for simplicity\n",
    "        \"\"\"\n",
    "        serialized_audio = inputs['source_serialized_audio'][0]\n",
    "        audio_buffer = io.BytesIO(base64.b64decode(serialized_audio))\n",
    "        audio, self.framerate = soundfile.read(audio_buffer)\n",
    "        if len(audio.shape) > 1 and audio.shape[1] > 1:\n",
    "            audio = audio[:, 0] #Get single channel  \n",
    "        wave_file = \"/phoenix/mlflow/tmp/{}.wav\".format(self.file_id)\n",
    "        soundfile.write(wave_file, audio, self.framerate)\n",
    "        text = self.asr_model.cuda().transcribe([wave_file])\n",
    "        return text\n",
    "\n",
    "    def text_to_audio(self, text):\n",
    "        \"\"\"\n",
    "        Generates audio from text using TTS templates.\n",
    "        \"\"\"\n",
    "        parsed = self.spectrogram_generator.cuda().parse(text)\n",
    "        spectrogram = self.spectrogram_generator.cuda().generate_spectrogram(tokens=parsed, speaker=2)\n",
    "        audio = self.vocoder.cuda().convert_spectrogram_to_audio(spec=spectrogram)\n",
    "        return audio.to('cpu').detach().numpy()\n",
    "\n",
    "    def serialize_audio(self, audio_np):\n",
    "        \"\"\"\n",
    "        Serializes an audio NumPy array to a base64 string representing a WAV file.\n",
    "        \"\"\"\n",
    "        audio_base64 = \"\"\n",
    "        wave_file = \"/phoenix/mlflow/tmp/out_{}.wav\".format(self.file_id)\n",
    "        soundfile.write(wave_file, audio_np, samplerate=41000, format='WAV')\n",
    "        \n",
    "        with io.BytesIO() as audio_buffer:\n",
    "            soundfile.write(audio_buffer, audio_np, samplerate=41000, format='WAV')\n",
    "            audio_buffer.seek(0)\n",
    "            audio_output = audio_buffer.read()\n",
    "        return audio_output\n",
    "\n",
    "    def load_context(self, context):\n",
    "        model_name=context.artifacts[\"model\"]\n",
    "        self.asr_model = nemo_asr.models.EncDecCTCModel.restore_from(\"{}/enc_dec_CTC.nemo\".format(model_name))\n",
    "        self.nmt_model = nemo_nlp.models.MTEncDecModel.restore_from(\"{}/MT_enc_dec.nemo\".format(model_name))\n",
    "        self.spectrogram_generator = nemo_tts.models.FastPitchModel.restore_from(\"{}/fast_pitch.nemo\".format(model_name))\n",
    "        self.vocoder = nemo_tts.models.HifiGanModel.restore_from(\"{}/hifi_gan.nemo\".format(model_name))\n",
    "        self.framerate = 41000\n",
    "        if not os.path.isdir(\"/phoenix/mlflow/tmp\"):\n",
    "            os.mkdir(\"/phoenix/mlflow/tmp\")\n",
    "         \n",
    "    def predict(self, context, model_input, params):\n",
    "        source_text = \"\"\n",
    "        self.file_id = uuid.uuid1()\n",
    "        if params[\"use_audio\"]:\n",
    "            source_text = self.transcribe_audio(model_input)[0]\n",
    "        else:\n",
    "            source_text = model_input['source_text'][0]\n",
    "        translated_text = self.nmt_model.cuda().translate([source_text])[0]\n",
    "        translated_audio = \"\"\n",
    "        if params[\"use_audio\"]:\n",
    "            audio = self.text_to_audio(translated_text)\n",
    "            translated_audio = self.serialize_audio(audio[0])\n",
    "        return {\"original_text\": source_text, \"translated_text\": translated_text, \"translated_serialized_audio\": translated_audio}\n",
    "\n",
    "    @classmethod\n",
    "    def log_model(cls, model_name, nemo_models, demo_folder): #eg (model, '', 'my_model')\n",
    "        import os, shutil\n",
    "        input_schema = Schema(\n",
    "            [\n",
    "                ColSpec(\"string\", \"source_text\"),\n",
    "                ColSpec(\"string\", \"source_serialized_audio\"),\n",
    "            ]\n",
    "        )\n",
    "        output_schema = Schema(\n",
    "            [\n",
    "                ColSpec(\"string\", \"original_text\"),\n",
    "                ColSpec(\"string\", \"translated_text\"),\n",
    "                ColSpec(\"string\", \"translated_serialized_audio\"),\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        params_schema = ParamSchema(\n",
    "            [\n",
    "                ParamSpec(\"use_audio\", \"boolean\", False)\n",
    "            ]\n",
    "        )\n",
    "      \n",
    "        signature = ModelSignature(inputs=input_schema, outputs=output_schema, params=params_schema)\n",
    "\n",
    "        if not os.path.isdir(model_name):\n",
    "            os.mkdir(model_name)\n",
    "        if \"enc_dec_CTC\" in nemo_models:\n",
    "            nemo_models[\"enc_dec_CTC\"].save_to(\"{}/enc_dec_CTC.nemo\".format(model_name))\n",
    "        if \"MT_enc_dec\" in nemo_models:\n",
    "            nemo_models[\"MT_enc_dec\"].save_to(\"{}/MT_enc_dec.nemo\".format(model_name))\n",
    "        if \"fast_pitch\" in nemo_models:\n",
    "            nemo_models[\"fast_pitch\"].save_to(\"{}/fast_pitch.nemo\".format(model_name))\n",
    "        if \"hifi_gan\" in nemo_models:\n",
    "            nemo_models[\"hifi_gan\"].save_to(\"{}/hifi_gan.nemo\".format(model_name))\n",
    "  \n",
    "        mlflow.pyfunc.log_model(\n",
    "            model_name,\n",
    "            python_model=cls(),\n",
    "            artifacts={\"model\": model_name, \"demo\": demo_folder},\n",
    "            signature=signature\n",
    "        )            \n",
    "        \n",
    "        shutil.rmtree(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(experiment_name='NeMo_translation')\n",
    "\n",
    "with mlflow.start_run(run_name='NeMo_en_es_translation') as run:\n",
    "    model_set = {\n",
    "        \"enc_dec_CTC\": asr_model,\n",
    "        \"MT_enc_dec\": nmt_model,\n",
    "        \"fast_pitch\": spectrogram_generator,\n",
    "        \"hifi_gan\": vocoder\n",
    "    }\n",
    "\n",
    "    NemoTranslationModel.log_model(model_name='nemo_en_es', nemo_models=model_set, demo_folder = \"demo\")\n",
    "    mlflow.register_model(model_uri = f\"runs:/{run.info.run_id}/nemo_en_es\", name=\"nemo_en_es\")\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "NeMo Getting Started",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
